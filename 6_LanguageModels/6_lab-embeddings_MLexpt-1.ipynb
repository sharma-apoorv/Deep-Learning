{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Lecture 6: Embeddings and Model Selection </h1> </center>\n",
    "<center> Krishna Pillutla, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n",
    "We will discuss two topics this lecture:\n",
    "- Embeddings for natural language\n",
    "- Model Selection with statistical tests\n",
    "\n",
    "\n",
    "The first part of this notebook has been adapted from the [D2L book]( http://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Embeddings for Natural Language\n",
    "\n",
    "The field of **natural language processing (NLP)** is concerned with the interaction between computers and natural (human) language. This involves \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "\n",
    "**Embeddings**:\n",
    "The use of machine learning for NLP, both in the classical settings as well as the modern deep learning era, have relied on *embedding* words in vector spaces.\n",
    "Words are made of characters, which are combinatorial in nature with no \"neighborhood\" structure which one expects of vectors in, say, a Euclidean space. \n",
    "The magic of embeddings is that they are able to capture some \"neighborhood\" structure in words, e.g., the embedding of synonyms are closer together than of words which have nothing in common. \n",
    "\n",
    "![](https://miro.medium.com/max/2400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "Image credits: https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
    "\n",
    "**Note**: Sometimes, we will work at the level of subword units, rather than words. Mathematically, the same treatment holds irrespective of how we *tokenize* the text. We will refer to these units as *tokens*.\n",
    "\n",
    "\n",
    "**Types of embeddings**:\n",
    "\n",
    "- Global embeddings: word2vec, GloVe\n",
    "- Contextual embeddings: ELMo, BERT, ...\n",
    "\n",
    "In this lab, we will play with the GloVe embeddings, which are global embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe embedding ~66M compressed + 164M uncompressed\n",
    "# Note to Windows users: Open the link below and unzip it in the same directory as the notebook\n",
    "import os\n",
    "if 'glove.6B.50d' not in os.listdir():\n",
    "    !wget http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip\n",
    "    !unzip glove.6B.50d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index each word in our dictionary using integers. \n",
    "We store the following mappings:\n",
    "- word $\\to$ index\n",
    "- index $\\to$ word\n",
    "- index $\\to$ embedding of the corresponding word\n",
    "\n",
    "Words not in our dictionary are denoted using the `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeWordEmbedding:\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        with open('glove.6B.50d/vec.txt') as f: # use encoding='utf8' on widows\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, np.asarray(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        # \"tokens\" is a list of words\n",
    "        # use as object[tokens]\n",
    "        # map token -> index -> vector\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[np.asarray(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        # Use as object(tokens)\n",
    "        return self.__getitem__(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = GloVeWordEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by noting that the embedding of a word does not depend on its context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the embeddings of words:\n",
    "sentence1 = 'I love data science'\n",
    "embeddings1 = glove_embedding[sentence1.split()]  # using the __getitem__ method\n",
    "# alternatively: \n",
    "# embeddings1 = glove_embedding(sentence1.split())  # using the __call__ method\n",
    "print(embeddings1.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 50)\n"
     ]
    }
   ],
   "source": [
    "sentence2 = 'As a kid, I always wanted to study mathematics and science'\n",
    "embeddings2 = glove_embedding(sentence2.split())\n",
    "print(embeddings2.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the embeddings of both the time the word \"science\" appears\n",
    "e1 = embeddings1[-1]\n",
    "e2 = embeddings2[-1]\n",
    "print(np.linalg.norm(e1-e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the cosine similarity between word embeddings. \n",
    "Recall that the cosine similarity between two vectors $u, v \\in \\mathbb{R}^d$ is defined as \n",
    "$$\n",
    "    S_{\\cos}(u, v) = \\frac{\\langle u, v\\rangle}{\\|u\\|_2  \\, \\|v\\|_2} .\n",
    "$$\n",
    "\n",
    "For any pair of vectors, the cosine similarity is always \n",
    "between $-1$ and $1$. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the k nearest neighbors \n",
    "# as per cosine similarity\n",
    "def k_nearest_neighbors(population, query, k):\n",
    "    # population is a matrix of size (n, dim)\n",
    "    # query is a matrix of shape (1, dim)\n",
    "    # k is an integer\n",
    "    # return topk indices and topk values\n",
    "    cos = np.dot(population, query.reshape(-1,)) / (\n",
    "        np.sqrt(np.sum(population * population, axis=1) + 1e-9) *\n",
    "        np.linalg.norm(query.reshape(-1))\n",
    "    )\n",
    "    topk_idx = np.argpartition(cos, -k)[-k:] # unsorted\n",
    "    topk_idx = topk_idx[np.argsort(cos[topk_idx])][::-1]  # sorted\n",
    "    topk_val = cos[topk_idx]\n",
    "    return topk_idx, topk_val\n",
    "# Hint for topk in numpy: https://stackoverflow.com/a/23734295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 microwave 0.9999999999839023\n",
      "1 analog 0.7300107691175304\n",
      "2 microwaves 0.7264979322621883\n",
      "3 oven 0.7115686481570181\n",
      "4 refrigerator 0.7039825402692077\n"
     ]
    }
   ],
   "source": [
    "query = glove_embedding(['microwave'])\n",
    "# Find the top-5 neighbors of \"microwave\" using the k_nearnest_neighbors function you wrote above\n",
    "# Also print the cosine similarity between the word you find and the query word\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 5)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 serendipity 0.9999999999485274\n",
      "1 profundity 0.7222786942557122\n",
      "2 happenstance 0.7016944428733708\n",
      "3 strangeness 0.6975966805924639\n",
      "4 weirdness 0.6901399894427984\n"
     ]
    }
   ],
   "source": [
    "# same as above, but with a different word\n",
    "query = glove_embedding(['serendipity'])\n",
    "\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 5)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies with word embeddings\n",
    "\n",
    "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, “man”:“woman”::“son”:“daughter” is an example of analogy, “man” is to “woman” as “son” is to “daughter”. \n",
    "\n",
    "The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a:b::c:d$, given the first three words, a, b and c, we want to find d. \n",
    "\n",
    "Assume the word vector for the word w is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)−\\text{vec}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c):\n",
    "    # Implement the analogy from above\n",
    "    vecs = glove_embedding[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = k_nearest_neighbors(glove_embedding.idx_to_vec, x, 1)\n",
    "    return glove_embedding.idx_to_token[int(topk[0])]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capital-country\n",
    "get_analogy('london', 'england', 'paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looked'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tense\n",
    "get_analogy('dance', 'danced', 'look')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection with Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a test to compare two different models. This is called the McNemar's test. \n",
    "\n",
    "Let $h_1$ and $h_2$ be two different classification algorithms. \n",
    "The hypotheses we're testing are:\n",
    "$$\n",
    "H_0 : \\quad \\text{acc}(h_1) = \\text{acc}(h_2) \\\\\n",
    "H_1 : \\quad \\text{acc}(h_1) \\ne \\text{acc}(h_2) ,\n",
    "$$\n",
    "where acc$(h)$ is the accuracy of the classifier $h$.\n",
    "\n",
    "\n",
    "To distinguish between the two, we compute two the following numbers:\n",
    "- $N_{01}$: the number of validation examples misclassified by $h_1$ but correctly classified by $h_2$\n",
    "- $N_{10}$: the number of validation examples correctly classified by $h_1$ but misclassified by $h_2$. \n",
    "\n",
    "The test statistic is \n",
    "$$\n",
    "    T = \\frac{(|N_{01} - N_{10}| - 1)^2}{N_{10} + N_{01}}.\n",
    "$$\n",
    "Its asymptotic distribution under the null is $\\chi^2$-distribution with $1$ degree of freedom. We reject the test null hypothesis if \n",
    "$$T > \\chi^2_{1, \\alpha}$$\n",
    "the $(1-\\alpha)$-quantile of the $\\chi^2_1$ distribution.\n",
    "\n",
    "**The exercise**:\n",
    "Run this hypothesis test to compare the MLP from week 1 and the ConvNet from week 2. Use a significance $\\alpha=0.01$. Train each network with SGD for $30$ passes with an appropriate learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, hidden_width=32):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(784, hidden_width)\n",
    "        self.linear2 = torch.nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = self.conv_ensemble_2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(model, X, y):\n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "def sgd_one_pass(model, X, y, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective(model, X[idx:idx+1], y[idx:idx+1]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        for (w, g) in zip(model.parameters(), gradients):\n",
    "            w.data -= learning_rate * g.data\n",
    "      \n",
    "    \n",
    "from tqdm.auto import trange # range + progress bar\n",
    "def sgd_n_passes(model, X_train, y_train, X_val, y_val, n_passes, learning_rate):\n",
    "    for i in trange(n_passes):\n",
    "        sgd_one_pass(model, X_train, y_train, learning_rate)\n",
    "    return compute_prediction_performance(model, X_val, y_val)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_prediction_performance(model, X, y):\n",
    "    # return a boolean vector of the same length as y\n",
    "    # each entry is True if correctly predicted, else False\n",
    "    # TODO: your code here\n",
    "    y_pred = torch.argmax(model(X), dim=1)\n",
    "    return (y_pred == y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b99afdd9344c2082f75c555a6f5d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = MLP()\n",
    "performance1 = sgd_n_passes(\n",
    "    model1, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef395db2d1c54cd5be1e88854e150025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = ConvNet()\n",
    "performance2 = sgd_n_passes(\n",
    "    model2, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2.5e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of MLP: 0.8336\n",
      "accuracy of ConvNet: 0.8713\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of MLP:', performance1.sum().item()/y_test.shape[0])\n",
    "print('accuracy of ConvNet:', performance2.sum().item()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test whether this difference is statistically significant or not. From a \n",
    "first glance, it does appear to be statistically significant as the gap is quite large. \n",
    "\n",
    "Compute $N_{01}$ and $N_{10}$ from the output of SGD above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N01 = (~performance1 & performance2).sum().item()  # MLP is wrong but ConvNet is correct\n",
    "N10 = (performance1 & ~performance2).sum().item()  # MLP is correct but ConvNet is wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the test statistic, the threshold, which is the $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution and read off the conclusion of the test. Recall that we have $\\alpha=0.01$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 122.8288444830582, threshold: 6.6348966010212145\n",
      "Null rejected\n"
     ]
    }
   ],
   "source": [
    "T = (abs(N01 - N10) - 1)**2 / (N01 + N10)  # statistic\n",
    "threshold = scipy.stats.chi2(df=1).ppf(0.99)\n",
    "\n",
    "print(f'Test statistic: {T}, threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Null rejected')\n",
    "else:\n",
    "    print('Failed to reject the null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Sentiment Analysis using GloVe Embeddings and Linear Models\n",
    "\n",
    "The goal of this exercise is to perform sentiment analysis using the data from the demo, except using GloVe embeddings. Please see this week's demo for details on the data. \n",
    "\n",
    "\n",
    "Download the data from [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip). Note that you will need an active Kaggle account to access the data. \n",
    "\n",
    "We will use movie reviews from Rotten Tomatoes. The sentiment labels are:\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SentenceId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase  Sentiment\n",
       "SentenceId                                                              \n",
       "1           A series of escapades demonstrating the adage ...          1\n",
       "2           This quiet , introspective and entertaining in...          4\n",
       "3           Even fans of Ismail Merchant 's work , I suspe...          1\n",
       "4           A positively thrilling combination of ethnogra...          3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filename = './data/sentiment-analysis-train.tsv'\n",
    "# keep one example per sentence (original data labels each phrase)\n",
    "data = pd.read_csv(filename, sep='\\t').groupby('SentenceId').first()\n",
    "data = data.drop(columns=['PhraseId'])\n",
    "\n",
    "\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split and featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 2) (1529, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1)  # shuffle\n",
    "train_data = data[:7000]\n",
    "test_data = data[7000:]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\varphi(w)$ denote the embedding of word $w$.\n",
    "Recall that GloVe is a global embedding which does not depend on the context of the word. \n",
    "\n",
    "For a piece of text denoted by words $T = (w_1, \\cdots, w_n)$, we \n",
    "summarize it by the vector \n",
    "$$\n",
    "    \\psi(T) := \\frac{1}{n} \\sum_{i=1}^n w_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "# Download the pre-trained model + tokenizer (a total of 440 MB)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # to tokenize the text\n",
    "model = BertModel.from_pretrained(model_name)  # PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def featurize(x): # x is pd.Series with text\n",
    "    features = []\n",
    "    for sen in tqdm(x):\n",
    "        sen = tokenizer.encode(sen, return_tensors='pt')\n",
    "        outputs = model(sen, return_dict=True)\n",
    "        embeddings = outputs.last_hidden_state.squeeze() # (len, dim)\n",
    "        mean_embedding = embeddings.mean(axis=0)\n",
    "        features.append(mean_embedding.numpy())\n",
    "    return np.stack(features)  # (n, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7000/7000 [13:47<00:00,  8.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1529/1529 [02:36<00:00,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = featurize(train_data['Phrase'])\n",
    "y_train = train_data['Sentiment'].values\n",
    "\n",
    "x_test = featurize(test_data['Phrase'])\n",
    "y_test = test_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple logistic regression classifier to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reduce the data dimensionality\n",
    "# This step is optional and only perform it if you \n",
    "# find that it helps the test accuracy\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, random_state=1).fit(x_train)  # keep 99% of the explained variance\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorvsharma/.local/lib/python3.8/site-packages/numpy/core/function_base.py:277: RuntimeWarning: overflow encountered in power\n",
      "  return _nx.power(base, y)\n",
      "/home/apoorvsharma/anaconda3/envs/data598/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/apoorvsharma/anaconda3/envs/data598/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/apoorvsharma/anaconda3/envs/data598/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/apoorvsharma/anaconda3/envs/data598/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/apoorvsharma/anaconda3/envs/data598/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv=5, random_state=0, Cs=np.logspace(1e-4, 1e3, 20), max_iter=200).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6147142857142858\n",
      "Test accuracy: 0.45062132112491826\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "print('Train accuracy:', (y_train_pred == y_train).mean())\n",
    "print('Test accuracy:', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00023029e+000, 4.28226643e+052, 1.83335838e+105, 7.84912152e+157,\n",
       "       3.36042910e+210, 1.43869396e+263,             inf,             inf,\n",
       "                   inf,             inf,             inf,             inf,\n",
       "                   inf,             inf,             inf,             inf,\n",
       "                   inf,             inf,             inf,             inf])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.Cs_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
