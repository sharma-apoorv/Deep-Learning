{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Multilayer Perceptrons and Stochastic Gradient Descent </h1></center>\n",
    "<center> Krishna Pillutla, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: PyTorch Preliminaries\n",
    "\n",
    "Use PyTorch's automatic differentiation routines to compute the following derivatives/gradients and verify them on sample inputs. The solution to the first one is provided as an example.\n",
    "\n",
    "1. $f : \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x) = \\|x\\| = \\sqrt{x_1^2 + x_2^2 + x_3^2}$. Find $\\nabla_x f(x)$ for some $x\\neq 0$ (note that $f$ is not differentiable at $0$).\n",
    "1. $g: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $g(y) = \\frac{\\exp(y_1)}{\\exp(y_1) + \\exp(y_2) + \\exp(y_3)}$. Find $\\nabla_y g(y)$.\n",
    "1. $h: \\mathbb{R}^2 \\times \\mathbb{R}^3 \\to \\mathbb{R}$ with $h(a, b) = a^\\top M b$ and $M\\in \\mathbb{R}^{2 \\times 3}$ is the matrix of all ones (i.e., $M_{ij}=1$ for each $i, j$). Find $\\nabla_a h(a, b)$ and $\\nabla_b h(a, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5410, -0.2934, -2.1788], requires_grad=True) tensor([ 0.5740, -0.1093, -0.8115]) tensor([ 0.5740, -0.1093, -0.8115], grad_fn=<DivBackward0>)\n",
      "tensor([ 1.5410, -0.2934, -2.1788], requires_grad=True) tensor([ 0.5740, -0.1093, -0.8115]) tensor([ 0.5740, -0.1093, -0.8115], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Solution to Part 1 in two ways:\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "true_gradient = x / torch.norm(x)  # computed analytically for verification\n",
    "\n",
    "# (a) Using the `.backward()` call\n",
    "output = torch.norm(x)\n",
    "output.backward()\n",
    "gradient = x.grad \n",
    "print(x, gradient, true_gradient)\n",
    "\n",
    "# (b) Using the `torch.autograd.grad` call\n",
    "output = torch.norm(x)\n",
    "# `torch.autograd.grad` returns a list. It has only one entry in this example\n",
    "gradient = torch.autograd.grad(outputs=output, inputs=[x])[0]\n",
    "print(x, gradient, true_gradient)\n",
    "\n",
    "## Solution to Part 2\n",
    "# <Your code here>\n",
    "\n",
    "## Solution to Part 3\n",
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generalize these examples to high dimensions.\n",
    "\n",
    "1. $f : \\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(x) = \\|x\\| = \\sqrt{\\sum_{j=1}^n x_j^2}$. Find $\\nabla_x f(x)$ for some $x\\neq 0$ (note that $f$ is not differentiable at $0$).\n",
    "1. $g: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $g(y) = \\exp(y_1)/\\sum_{k=1}^n \\exp(y_k)$. Find $\\nabla_y g(y)$.\n",
    "1. $h: \\mathbb{R}^m \\times \\mathbb{R}^n \\to \\mathbb{R}$ with $h(a, b) = a^\\top M b$ and $M\\in \\mathbb{R}^{m \\times n}$ is the matrix of all ones (i.e., $M_{ij}=1$ for each $i\\in[m],j\\in[n]$). Find $\\nabla_a h(a, b)$ and $\\nabla_b h(a, b)$.\n",
    "\n",
    "For these examples, you may take $n = 100$ and $m = 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution to Part 1 \n",
    "# <Your code here>\n",
    "\n",
    "## Solution to Part 2\n",
    "# <Your code here>\n",
    "\n",
    "## Solution to Part 3\n",
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's automatic differentiation works by building a \n",
    "\"computation graph\" of all operations on tensors with `requires_grad=True`. \n",
    "It then computes the requested gradients by performing \n",
    "reverse-mode automatic differentiation, a.k.a. backpropagation. \n",
    "\n",
    "![Computational graph](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
    "\n",
    "There are number of caveats to keep in mind:\n",
    "\n",
    "- Automatic differentiation only works for floating point types (float32, float64). It does not work for integer types. (Think about why this is the case).\n",
    "- \"Building a computation graph\" takes up space. The memory usage increases proportional to the number of intermediate quantities stored.\n",
    "\n",
    "There are number of operations one might want to perform without building a computational graph. Examples include:\n",
    "\n",
    "- SGD updates\n",
    "- Accuracy computation (in general, logging utilies)\n",
    "\n",
    "PyTorch allows you to \"hide\" computations from autograd using\n",
    "`torch.no_grad()` (for example, see the accuracy computation functions below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: SGD for logistic regression versus the learning rate\n",
    "\n",
    "We will run SGD on the same fashion MNIST dataset as in the demo. Our goal here is to study the effect of the learning rate on the train/test loss/accuracy.\n",
    "\n",
    "Your task is to fill in the SGD update (look for `# <Your code here>`) and make the requested plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('../data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, idx in enumerate(np.random.choice(X_train.shape[0], 5)):\n",
    "    ax[i].imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)\n",
    "    ax[i].set_title(f'Label = {y_train[idx]}', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(w, X, y, reg_param):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        Require w of shape (d,K), X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = torch.matmul(X, w)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return (\n",
    "        cross_entropy(input=score, target=y, reduction='mean') \n",
    "        + 0.5 * reg_param * torch.norm(w)**2\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(w, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        Require w of shape (d,K), X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = torch.matmul(X, w)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    # <Your code here>: compute the accuraacy from predictions and y\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(w, reg_param, verbose=False):\n",
    "    train_loss = compute_objective(w, X_train, y_train, reg_param)\n",
    "    test_loss = compute_objective(w, X_test, y_test, reg_param)\n",
    "    train_accuracy = compute_accuracy(w, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(w, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_one_pass(w, X, y, reg_param, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1    \n",
    "        objective = compute_objective(w, X[idx:idx+1], y[idx:idx+1], reg_param) \n",
    "        \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradient = torch.autograd.grad(outputs=objective, inputs=w)[0]\n",
    "        \n",
    "        # perform SGD update\n",
    "        with torch.no_grad():\n",
    "            # <Your code here>\n",
    "            w -= learning_rate * gradient\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the divergent learning rate. \n",
    "\n",
    "Recall the definition of the divergent learning rate. We say ùúÇ‚ãÜ is the divergent learning rate if SGD with a learning rate of 2ùúÇ‚ãÜ diverges, but SGD with a learning rate of ùúÇ‚ãÜ does not. \n",
    "\n",
    "Edit the code below to change the learning rate. \n",
    "\n",
    "*Hint 1*: Try different orders of magnitude. For instance, start with 1e-2. If it diverges, try 1e-6, if not, try 10.0. Once we establish a lower and upper bound on it, finding the divergent learning rate comes down to a binary search (on a logarithmic scale). \n",
    "\n",
    "*Hint 2*: A common strategy is to search in powers of 10 (as in hint 1), and then narrow the search down in powers of 2. For instance, if we end up with 1e-3 as an estimate of the divergent learning from hint 1, try out 2e-3 and 4e-3 as well in order to refine the estimate of the divergent learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 2.303, Train Accuracy = 0.098, Test Loss = 2.303, Test Accuracy = 0.100\n",
      "103.15893137593865\n",
      "194.79042709124374\n",
      "174.0596806019119\n",
      "141.78528749362326\n",
      "132.42654678203917\n",
      "106.42317721592987\n",
      "131.61351551665717\n",
      "128.1441627998331\n",
      "174.3994383050109\n",
      "150.2089771425357\n",
      "143.28154570446577\n",
      "116.40113943699606\n",
      "199.8764164583908\n",
      "162.68606289290014\n",
      "151.27317784172155\n",
      "173.9105923320763\n",
      "147.4676345129256\n",
      "104.297217347818\n",
      "130.45124485936688\n",
      "100.7929389549857\n",
      "98.82500379839146\n",
      "112.11288128118325\n",
      "104.64784034497298\n",
      "109.23281020146939\n",
      "84.35312201571814\n",
      "150.70716382882793\n",
      "168.08975698319122\n",
      "151.72776730102532\n",
      "119.67584303539502\n",
      "92.21133166979703\n",
      "168.43513152606653\n",
      "145.6497441073821\n",
      "134.78829310470508\n",
      "100.71316010690327\n",
      "159.18651719107805\n",
      "120.81475018933796\n",
      "99.1330247839371\n",
      "89.65528010914447\n",
      "128.70227926490367\n",
      "111.50617798468055\n",
      "129.6882111704976\n",
      "115.36885023431864\n",
      "90.17742399459306\n",
      "130.61416277401045\n",
      "136.57004163824158\n",
      "135.63897981414567\n",
      "100.20741731452547\n",
      "101.61594001401618\n",
      "123.71939997569494\n",
      "119.38155471045563\n",
      "141.1719363969387\n",
      "159.15885930523754\n",
      "118.54768848099576\n",
      "108.18022034999888\n",
      "100.8861016539583\n",
      "127.82286736145238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fbb68a36f0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd_one_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7dc4635a0c30>\u001b[0m in \u001b[0;36msgd_one_pass\u001b[0;34m(w, X, y, reg_param, learning_rate, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# compute the objective.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Note: This function requires X to be of shape (n,d). In this case, n=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maverage_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-357d25d675bc>\u001b[0m in \u001b[0;36mcompute_objective\u001b[0;34m(w, X, y, reg_param)\u001b[0m\n\u001b[1;32m      8\u001b[0m     return (\n\u001b[1;32m      9\u001b[0m         \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_param\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "reg_param = 0.0\n",
    "\n",
    "w = torch.zeros(784, n_class, requires_grad=True)\n",
    "_ = compute_logs(w, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "w = sgd_one_pass(w, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs(w, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the following function which runs SGD for some number of passes through the data and tracks the test accuracy at the end of each pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: X_train, y_train, X_test, y_test are global variables\n",
    "def run_sgd(w, reg_param, learning_rate, num_passes=20, verbose=True):\n",
    "    logs = []\n",
    "\n",
    "    log_statistics = # <Your code here>; \n",
    "    #call `compute_logs` and track the train/test loss/accuracy\n",
    "    logs.append(log_statistics)\n",
    "    for j in range(num_passes):\n",
    "        # <Your code here>. Hint: call sgd_one_pass\n",
    "        log_statistics = # <Your code here>; \n",
    "        #call `compute_logs` and track the train/test loss/accuracy\n",
    "        logs.append(log_statistics)\n",
    "    return w, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD for the learning rates from the list `[1e-2, 1e-3, 1e-4, 1e-5]` for 10 epochs. \n",
    "Plot 4 curves:\n",
    "\n",
    " - Train loss vs number of passes,\n",
    " - Train accuracy vs number of passes,\n",
    " - Test loss vs number of passes,\n",
    " - Test accuracy vs number of passes.\n",
    " \n",
    " Each curve must contain multiple lines, each corresponding to a different learning rate. \n",
    " You may use `reg_param=1e-3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The effect of initialization and learning rate on MLPs\n",
    "We will repeat the same experiment with MLPs.\n",
    "\n",
    "Start by filling in the blank in the `sgd_one_pass` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to compute the objective and the accuracy\n",
    "\n",
    "def mlp(X, ws, bs):\n",
    "    hidden = X # (n, d_0)\n",
    "    for w, b in zip(ws[:-1], bs[:-1]):\n",
    "        hidden = torch.matmul(hidden, w) + b[None, :]  # (n, d_{j-1}) * (d_{j-1}, d_j) = (n, d_j)\n",
    "        hidden = torch.nn.functional.relu(hidden)\n",
    "    return torch.matmul(hidden, ws[-1]) + bs[-1][None, :]\n",
    "\n",
    "def compute_objective_mlp(ws, bs, X, y, reg_param):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        ws is a list of tensors of consistent shapes,\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = mlp(X, ws, bs)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return (\n",
    "        cross_entropy(input=score, target=y, reduction='mean') \n",
    "        + 0.5 * reg_param * sum([torch.norm(w)**2 for w in ws])\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_mlp(ws, bs, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = mlp(X, ws, bs)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs_mlp(ws, bs, reg_param, verbose=False):\n",
    "    train_loss = compute_objective_mlp(ws, bs, X_train, y_train, reg_param)\n",
    "    test_loss = compute_objective_mlp(ws, bs, X_test, y_test, reg_param)\n",
    "    train_accuracy = compute_accuracy_mlp(ws, bs, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy_mlp(ws, bs, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)\n",
    "\n",
    "def sgd_one_pass_mlp(ws, bs, X, y, reg_param, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective_mlp(ws, bs, X[idx:idx+1], y[idx:idx+1], reg_param) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        all_parameters = [*ws, *bs]\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=all_parameters)\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        with torch.no_grad():\n",
    "            # TODO: <Your code here>\n",
    "    return ws, bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the divergent learning rate. Edit the code below to change the learning rate and see what you observe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "hidden_size = 1024\n",
    "ws = [1e-6 * torch.randn(784, hidden_size, requires_grad=True),\n",
    "      1e-6 * torch.randn(hidden_size, n_class, requires_grad=True)]\n",
    "bs = [torch.zeros(hidden_size, requires_grad=True),\n",
    "      torch.zeros(n_class, requires_grad=True)]\n",
    "\n",
    "_ = compute_logs_mlp(ws, bs, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "ws, bs = sgd_one_pass_mlp(ws, bs, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs_mlp(ws, bs, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will play with the initialization.\n",
    "\n",
    "- Try initializing with every W_j, b_j be 0 everywhere. What happens?\n",
    "- Try W_j, b_j be 1 everywhere. What happens?\n",
    "\n",
    "For these two cases, compute the gradient $\\nabla f(w)$. What do you observe? Does this explain why these initializations are unsuitable for stochastic gradient optimization?\n",
    "\n",
    "- Try increasing the variance from 0 to 1e-20, 1e-10, 1e-6, 1e-3 and 1e-1. What do you observe?\n",
    "- Try out the so-called Glorot-Bengio initialization scheme, where $W_j \\in \\mathbb{R}^{d_{j-1}\\times d_j}$ is taken to be be normally distributed with variance $1/d_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-3\n",
    "\n",
    "# TODO: change the initialization\n",
    "ws = [1e-6 * torch.randn(784, hidden_size, requires_grad=True),\n",
    "      1e-6 * torch.randn(hidden_size, n_class, requires_grad=True)]\n",
    "bs = [torch.zeros(hidden_size, requires_grad=True),\n",
    "      torch.zeros(n_class, requires_grad=True)]\n",
    "_ = compute_logs(ws, bs, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "ws = sgd_one_pass(ws, bs, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs(ws, bs, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we a function which makes a certain number of passes of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgd_mlp(ws, bs, reg_param, learning_rate, num_passes=10, verbose=True):\n",
    "    logs = [compute_logs_mlp(ws, bs, reg_param, verbose=verbose)]\n",
    "    for j in range(num_passes):\n",
    "        ws, bs = sgd_one_pass_mlp(ws, bs, X_train, y_train, \n",
    "                          reg_param, learning_rate, verbose=False)\n",
    "        logs.append(compute_logs_mlp(ws, bs, reg_param, verbose=verbose))\n",
    "    return ws, bs, np.asarray(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD for the learning rates from the list `[1e-2, 1e-3, 1e-4, 1e-5]` for 10 epochs. \n",
    "Plot 4 curves:\n",
    "\n",
    " - Train loss vs number of passes,\n",
    " - Train accuracy vs number of passes,\n",
    " - Test loss vs number of passes,\n",
    " - Test accuracy vs number of passes.\n",
    " \n",
    " Each curve must contain multiple lines, each corresponding to a different learning rate. \n",
    " You may use `reg_param=0`. \n",
    " \n",
    " Note: This takes several minutes to run. It took me 15 min for everything to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: <Your code here>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
